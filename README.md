# ðŸ§  NanoInfer  
> The smallest, clearest, and most educational inference pipeline for LLMs.  
> **From checkpoint to conversation â€” in 100 lines of code.**

---

## âœ¨ What is NanoInfer?

**NanoInfer** æ˜¯ä¸€ä¸ªæžç®€æŽ¨ç†æ¡†æž¶ / æ•™ç¨‹ï¼Œæ—¨åœ¨ä»Žåº•å±‚å‰–æž **LLM æŽ¨ç†é˜¶æ®µ** çš„å…¨è¿‡ç¨‹ã€‚  
å®ƒç»§æ‰¿äº† [NanoChat](https://github.com/karpathy/nanochat) çš„ç†å¿µï¼š  
> â€œTrain small, understand deeply.â€

NanoInfer è®©ä½ ä»…ä½¿ç”¨ PyTorch ç†è§£ä¸€ä¸ªå®Œæ•´çš„è‡ªå›žå½’æŽ¨ç†å¾ªçŽ¯ï¼š  
- å¦‚ä½•ä»Ž HFæ¨¡åž‹ æˆ– `.safetensors` æƒé‡æ–‡ä»¶åŠ è½½æ¨¡åž‹  
- å¦‚ä½•æ‰§è¡Œ `generate()` æŽ¨ç†å¾ªçŽ¯ï¼ˆå« KV cacheï¼‰  
- å¦‚ä½•æž„å»º tokenizer + sampling  
- å¦‚ä½•åšæŽ¨ç†è¯„ä¼°ä¸Žéƒ¨ç½²  

ç›®æ ‡ï¼š**ç†è§£æŽ¨ç† â†’ ä¼˜åŒ–æŽ¨ç† â†’ æœåŠ¡æŽ¨ç†**

---

## ðŸ§© Features

| æ¨¡å— | åŠŸèƒ½ | æŠ€æœ¯å…³é”®è¯ |
|------|------|-------------|
| ðŸ§  Model Loader | æ”¯æŒ HuggingFace å’Œ .pt æ ¼å¼ | HuggingFace / PyTorch |
| ðŸ”¡ Tokenizer | å…¼å®¹ HuggingFace tokenizer | AutoTokenizer / BPE |
| ðŸ” Generator | è‡ªå›žå½’ç”Ÿæˆ | KV Cache / Sampling |
| ðŸ“Š Evaluator | è®¡ç®— PPL / BLEU / CORE | è½»é‡æŒ‡æ ‡è®¡ç®— |
| ðŸŒ Chat Server | å¿«é€Ÿéƒ¨ç½² Web èŠå¤© | FastAPI / HuggingFace |
| âš™ï¸ Accelerator | æ€§èƒ½ä¼˜åŒ– | FP16 / Torch Compile / CUDA Graphs |

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/yourname/nanoinfer.git
cd nanoinfer
pip install -r requirements.txt
````

Requirements:

* Python â‰¥ 3.10
* PyTorch â‰¥ 2.1
* transformers, numpy, tqdm, fastapi, uvicorn

**æ–°å¢ž**: çŽ°åœ¨é»˜è®¤æ”¯æŒ HuggingFace æ¨¡åž‹ï¼Œæ— éœ€è½¬æ¢ï¼

---

## ðŸš€ Quick Start

è¿è¡Œå•è½®æŽ¨ç†ï¼ˆæœ€å°ç¤ºä¾‹ï¼‰ï¼š

```bash
# ä½¿ç”¨ HuggingFace æ¨¡åž‹ï¼ˆæŽ¨èï¼‰
python -m scripts.chat_infer --model gpt2 --prompt "Explain transformers in simple terms."

# ä½¿ç”¨ .pt æ£€æŸ¥ç‚¹æ–‡ä»¶
python -m scripts.chat_infer --model model.pt --prompt "Explain transformers in simple terms."

# CPU æŽ¨ç†
python -m scripts.chat_infer --model gpt2 --device cpu --dtype fp32 --prompt "Explain transformers in simple terms."
```

è¾“å‡ºï¼š

```
ðŸ§  Loading tokenizer and model...
ðŸ—£ï¸ Model output:
Transformers are neural networks that learn context by attending to all words at once.
â±ï¸ Inference time: 0.83s for 200 tokens
```

---


## ðŸ¤— HuggingFace æ¨¡åž‹æ”¯æŒ

NanoInfer çŽ°åœ¨é»˜è®¤æ”¯æŒ HuggingFace æ¨¡åž‹ï¼Œæ— éœ€ä»»ä½•è½¬æ¢ï¼

### æ”¯æŒçš„æ¨¡åž‹

- **GPT-2 ç³»åˆ—**: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **DialoGPT ç³»åˆ—**: `microsoft/DialoGPT-small`, `microsoft/DialoGPT-medium`
- **å…¶ä»– GPT æž¶æž„æ¨¡åž‹**

### å¿«é€Ÿä½¿ç”¨

```bash
# ç›´æŽ¥ä½¿ç”¨ HuggingFace æ¨¡åž‹
python -m scripts.chat_infer --model gpt2 --prompt "Hello world"

# ä½¿ç”¨æ›´å¤§çš„æ¨¡åž‹
python -m scripts.chat_infer --model gpt2-medium --prompt "The future of AI"

# å¯åŠ¨ API æœåŠ¡å™¨
python -m scripts.chat_server --model gpt2 --host 0.0.0.0 --port 8000
```

### ç¼–ç¨‹æŽ¥å£

```python
from nanoinfer.model.loader import load_model
from nanoinfer.tokenizer.tokenizer import Tokenizer

# åŠ è½½ HuggingFace æ¨¡åž‹
model, config, tokenizer_path = load_model("gpt2", device="cuda")
tokenizer = Tokenizer(tokenizer_path)

# ä½¿ç”¨æ¨¡åž‹è¿›è¡ŒæŽ¨ç†
input_ids = tokenizer.encode("Hello world", return_tensors="pt")
output = model.generate(input_ids, max_new_tokens=50)
response = tokenizer.decode(output[0])
```

---

## ðŸ§  Inference Pipeline

```python
# pseudo-code overview

from nanoinfer.model.loader import load_model
from nanoinfer.engine.generator import generate
from nanoinfer.tokenizer.tokenizer import Tokenizer

# åŠ è½½ HuggingFace æ¨¡åž‹ï¼ˆæŽ¨èï¼‰
model, config, tokenizer_path = load_model("gpt2", device="cuda")
tokenizer = Tokenizer(tokenizer_path)

# æˆ–è€…åŠ è½½ .pt æ£€æŸ¥ç‚¹
# model, config, tokenizer_path = load_model("model.pt", device="cuda")

prompt = "The theory of attention is"
ids = tokenizer.encode(prompt, return_tensors="pt")

out = generate(model, ids, max_new_tokens=128, temperature=0.8)
print(tokenizer.decode(out[0]))
```

æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š

* KV Cacheï¼ˆO(N) æŽ¨ç†ï¼‰
* Temperature / Top-p / Top-k é‡‡æ ·
* FP16 + CUDA Graphs åŠ é€Ÿ
* æ‰¹é‡ç”Ÿæˆï¼ˆBatch Decodeï¼‰

---

## ðŸ§ª Evaluation

```bash
# ä½¿ç”¨ HuggingFace æ¨¡åž‹è¿›è¡Œè¯„ä¼°
torchrun --standalone --nproc_per_node=1 -m scripts.chat_eval_infer \
  --model gpt2 \
  --eval_set ~/nanobase/eval_bundle/core_eval.json

# æˆ–ä½¿ç”¨ .pt æ£€æŸ¥ç‚¹
torchrun --standalone --nproc_per_node=1 -m scripts.chat_eval_infer \
  --model ~/nanobase/models/sft_model.pt \
  --eval_set ~/nanobase/eval_bundle/core_eval.json
```

Metrics:

| æŒ‡æ ‡                   | å«ä¹‰       |
| -------------------- | -------- |
| **Perplexity (PPL)** | è¯­è¨€å»ºæ¨¡å›°æƒ‘åº¦  |
| **CORE metric**      | å¤æ‚æŽ¨ç†ä»»åŠ¡è¯„ä¼° |
| **BLEU / ROUGE**     | è‡ªç„¶è¯­è¨€ç”Ÿæˆè´¨é‡ |

---

## âš¡ Performance Tips

| ä¼˜åŒ–é¡¹                 | æè¿°                | GPU æ•ˆæžœ      | CPU æ•ˆæžœ      |
| ------------------- | ----------------- | --------- | --------- |
| **KV cache**        | ç¼“å­˜æ³¨æ„åŠ›å±‚åŽ†å²çŠ¶æ€        | â±ï¸ 2â€“4Ã—   | â±ï¸ 2â€“3Ã—   |
| **FP16**            | åŠç²¾åº¦æŽ¨ç†             | ðŸ’¾ æ˜¾å­˜å‡åŠ   | âš ï¸ å¯èƒ½æ›´æ…¢   |
| **FP32**            | å•ç²¾åº¦æŽ¨ç†ï¼ˆCPU æŽ¨èï¼‰     | ðŸ’¾ æ˜¾å­˜æ›´å¤š   | ðŸš€ é€šå¸¸æ›´å¿«   |
| **torch.compile()** | PyTorch 2.x è‡ªåŠ¨å›¾ä¼˜åŒ– | ðŸš€ 20â€“40% | ðŸš€ 30â€“50% |
| **Batch Decode**    | å¤š prompt å¹¶è¡Œç”Ÿæˆ     | ðŸ” åžåæå‡   | ðŸ” åžåæå‡   |
| **CUDA Graphs**     | é¢„ç¼–è¯‘æŽ¨ç†å›¾ï¼ˆä»… GPUï¼‰    | âš™ï¸ ç¨³å®šå»¶è¿Ÿ   | âŒ ä¸æ”¯æŒ     |

---

## ðŸŒ Serve It

å¯åŠ¨äº¤äº’ç•Œé¢ï¼š

```bash
python -m scripts.chat_server
```

è®¿é—® ðŸ‘‰ [http://localhost:8000](http://localhost:8000)

ç¤ºä¾‹ï¼š

```python
POST /api/chat
{
  "prompt": "Write a haiku about GPU inference."
}
```

---

## ðŸ§­ Roadmap

* [x] åŸºç¡€æŽ¨ç†å¾ªçŽ¯ (CPU/GPU)
* [ ] CORE ä»»åŠ¡è¯„ä¼°
* [ ] FastAPI æœåŠ¡æŽ¥å£
* [ ] CPU ä¼˜åŒ–æ”¯æŒ
* [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
* [x] Streaming ç”ŸæˆæŽ¥å£
* [ ] Tensor Parallel Serving
* [ ] åŠ¨æ€ KV cache ç®¡ç†
* [ ] SGLang / vLLM åŽç«¯é›†æˆ

---

## ðŸ§© Relationship to NanoChat

| é¡¹ç›®                                               | åŠŸèƒ½é˜¶æ®µ | æ ¸å¿ƒç›®æ ‡       |
| ------------------------------------------------ | ---- | ---------- |
| [NanoChat](https://github.com/karpathy/nanochat) | è®­ç»ƒ   | ä»Žé›¶ç†è§£è¯­è¨€æ¨¡åž‹è®­ç»ƒ |
| **NanoInfer**                                    | æŽ¨ç†   | ä»Žé›¶ç†è§£è¯­è¨€æ¨¡åž‹æŽ¨ç† |

> â€œNanoChat teaches your model to think.
> NanoInfer teaches it to speak.â€

---

## ðŸ§‘â€ðŸ’» Philosophy

> **Clarity before performance.**
> We focus on transparency â€” not throughput.
> Understand the loop. Then optimize it.

---

## ðŸª„ License

MIT Â© 2025 [YourName]
Contributions are welcome!

---

## ðŸŒŸ Acknowledgements

Inspired by:

* [NanoGPT](https://github.com/karpathy/nanogpt)
* [NanoChat](https://github.com/karpathy/nanochat)
* [vLLM](https://github.com/vllm-project/vllm)
* [SGLang](https://github.com/sglang-ai/sglang)
