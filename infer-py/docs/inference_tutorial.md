# ğŸ§  NanoInfer æ¨ç†æ•™ç¨‹

> ä»é›¶ç†è§£ LLM æ¨ç†çš„å…¨è¿‡ç¨‹

## ç›®å½•

1. [æ¨ç†åŸç†](#æ¨ç†åŸç†)
2. [KV Cache æœºåˆ¶](#kv-cache-æœºåˆ¶)
3. [é‡‡æ ·ç­–ç•¥è¯¦è§£](#é‡‡æ ·ç­–ç•¥è¯¦è§£)
4. [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](#æ€§èƒ½ä¼˜åŒ–æŒ‡å—)
5. [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)

---

## æ¨ç†åŸç†

### è‡ªå›å½’ç”Ÿæˆ

LLM æ¨ç†çš„æ ¸å¿ƒæ˜¯**è‡ªå›å½’ç”Ÿæˆ**ï¼šæ¨¡å‹é€ä¸ªé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œç›´åˆ°ç”Ÿæˆç»“æŸã€‚

```python
# ä¼ªä»£ç ï¼šè‡ªå›å½’ç”Ÿæˆå¾ªç¯
def generate(model, prompt):
    tokens = tokenize(prompt)
    
    for i in range(max_length):
        # 1. å‰å‘ä¼ æ’­
        logits = model(tokens)
        
        # 2. é‡‡æ ·ä¸‹ä¸€ä¸ª token
        next_token = sample(logits[-1])
        
        # 3. æ·»åŠ åˆ°åºåˆ—
        tokens.append(next_token)
        
        # 4. æ£€æŸ¥ç»“æŸæ¡ä»¶
        if is_end_token(next_token):
            break
    
    return detokenize(tokens)
```

### å…³é”®æ¦‚å¿µ

- **Logits**: æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼Œè¡¨ç¤ºæ¯ä¸ª token çš„"å¯èƒ½æ€§"
- **Temperature**: æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼ˆ0.1=ç¡®å®šæ€§ï¼Œ1.0=æ ‡å‡†ï¼Œ2.0=é«˜éšæœºæ€§ï¼‰
- **Top-p (Nucleus)**: åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token ä¸­é‡‡æ ·
- **Top-k**: åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·

---

## KV Cache æœºåˆ¶

### é—®é¢˜ï¼šé‡å¤è®¡ç®—

åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯æ¬¡åªç”Ÿæˆä¸€ä¸ªæ–° tokenï¼Œä½†æ¨¡å‹éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ï¼š

```python
# æ²¡æœ‰ KV Cacheï¼šæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—
for i in range(3):
    # ç¬¬1æ¬¡ï¼šè®¡ç®— [token1] çš„æ³¨æ„åŠ›
    # ç¬¬2æ¬¡ï¼šè®¡ç®— [token1, token2] çš„æ³¨æ„åŠ›  â† é‡å¤è®¡ç®— token1
    # ç¬¬3æ¬¡ï¼šè®¡ç®— [token1, token2, token3] çš„æ³¨æ„åŠ›  â† é‡å¤è®¡ç®— token1, token2
```

### è§£å†³æ–¹æ¡ˆï¼šç¼“å­˜ Key å’Œ Value

```python
# æœ‰ KV Cacheï¼šåªè®¡ç®—æ–° token
past_key_values = None
for i in range(3):
    # åªè®¡ç®—æ–° token çš„ K, V
    new_k, new_v = compute_kv(new_token)
    
    # æ‹¼æ¥å†å² K, V
    k = concat([past_k, new_k])
    v = concat([past_v, new_v])
    
    # ç¼“å­˜ä¾›ä¸‹æ¬¡ä½¿ç”¨
    past_key_values = (k, v)
```

### å®ç°ç»†èŠ‚

```python
class CausalSelfAttention(nn.Module):
    def forward(self, x, past_key_values=None):
        # è®¡ç®—å½“å‰ token çš„ Q, K, V
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        
        # å¦‚æœæœ‰å†å² KVï¼Œæ‹¼æ¥
        if past_key_values is not None:
            past_k, past_v = past_key_values
            k = torch.cat([past_k, k], dim=2)  # æ‹¼æ¥æ—¶é—´ç»´åº¦
            v = torch.cat([past_v, v], dim=2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))
        att = att.masked_fill(self.bias == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        out = att @ v
        
        # è¿”å›è¾“å‡ºå’Œæ–°çš„ KV
        return out, (k, v)
```

### æ€§èƒ½æå‡

- **æ—¶é—´å¤æ‚åº¦**: O(NÂ²) â†’ O(N)
- **å®é™…åŠ é€Ÿ**: 2-4Ã— æ¨ç†é€Ÿåº¦æå‡
- **å†…å­˜å¼€é”€**: å¢åŠ  ~50% æ˜¾å­˜ä½¿ç”¨

---

## é‡‡æ ·ç­–ç•¥è¯¦è§£

### 1. Greedy é‡‡æ ·

é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenï¼š

```python
def greedy_sampling(logits):
    return torch.argmax(logits, dim=-1)
```

**ç‰¹ç‚¹**: ç¡®å®šæ€§ï¼Œé€‚åˆéœ€è¦ä¸€è‡´æ€§çš„åœºæ™¯

### 2. Temperature é‡‡æ ·

é€šè¿‡æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§ï¼š

```python
def temperature_sampling(logits, temperature=1.0):
    scaled_logits = logits / temperature
    probs = F.softmax(scaled_logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

**æ¸©åº¦æ•ˆæœ**:
- `temperature = 0.1`: å‡ ä¹ç¡®å®šæ€§ï¼Œé‡å¤æ€§é«˜
- `temperature = 1.0`: æ ‡å‡†éšæœºæ€§
- `temperature = 2.0`: é«˜éšæœºæ€§ï¼Œåˆ›æ„æ€§å¼º

### 3. Top-k é‡‡æ ·

åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·ï¼š

```python
def top_k_sampling(logits, k):
    values, indices = torch.topk(logits, k)
    logits[logits < values[:, -1:]] = float('-inf')
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

### 4. Top-p (Nucleus) é‡‡æ ·

åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token ä¸­é‡‡æ ·ï¼š

```python
def top_p_sampling(logits, p):
    # æ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    probs = F.softmax(sorted_logits, dim=-1)
    
    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumulative_probs = torch.cumsum(probs, dim=-1)
    
    # ç§»é™¤è¶…å‡ºé˜ˆå€¼çš„ token
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False
    
    # åº”ç”¨æ©ç 
    logits[sorted_indices_to_remove] = float('-inf')
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)
```

### é‡‡æ ·ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ç¡®å®šæ€§ | å¤šæ ·æ€§ | é€‚ç”¨åœºæ™¯ |
|------|--------|--------|----------|
| Greedy | é«˜ | ä½ | ä»£ç ç”Ÿæˆã€æ•°å­¦é—®é¢˜ |
| Temperature | ä¸­ | ä¸­ | é€šç”¨å¯¹è¯ |
| Top-k | ä¸­ | ä¸­ | å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ |
| Top-p | ä½ | é«˜ | åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ |

---

## æ€§èƒ½ä¼˜åŒ–æŒ‡å—

### 1. ç²¾åº¦ä¼˜åŒ–

#### GPU ä¼˜åŒ–
```python
# FP16 åŠç²¾åº¦ï¼ˆGPU æ¨èï¼‰
model = model.half()  # æ˜¾å­˜å‡åŠï¼Œé€Ÿåº¦æå‡ 1.5-2Ã—

# BF16 è„‘æµ®ç‚¹ï¼ˆGPU æ¨èï¼‰
model = model.bfloat16()  # æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§
```

#### CPU ä¼˜åŒ–
```python
# CPU ä¸Šä½¿ç”¨ FP32 ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
model = model.float()  # CPU ä¸Š FP32 é€šå¸¸æ¯” FP16 æ›´å¿«

# è®¾ç½® CPU çº¿ç¨‹æ•°
torch.set_num_threads(4)  # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
```

### 2. ç¼–è¯‘ä¼˜åŒ–

#### Torch Compile
```python
model = torch.compile(model)  # PyTorch 2.0+ è‡ªåŠ¨å›¾ä¼˜åŒ–
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# å•æ¡æ¨ç†
for prompt in prompts:
    generate(model, prompt)

# æ‰¹å¤„ç†æ¨ç†
batch_prompts = [prompt1, prompt2, prompt3]
generate_batch(model, batch_prompts)  # 3-5Ã— ååæå‡
```

### 4. å†…å­˜ä¼˜åŒ–

#### GPU å†…å­˜ç®¡ç†
```python
# æ¸…ç†æ˜¾å­˜
torch.cuda.empty_cache()

# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
print(f"Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
print(f"Reserved: {torch.cuda.memory_reserved() / 1024**2:.1f}MB")
```

#### CPU å†…å­˜ç®¡ç†
```python
# æ¸…ç† CPU å†…å­˜
import gc
gc.collect()

# ç›‘æ§ CPU å†…å­˜ä½¿ç”¨
import psutil
process = psutil.Process()
print(f"RSS: {process.memory_info().rss / 1024**2:.1f}MB")
print(f"VMS: {process.memory_info().vms / 1024**2:.1f}MB")
```

### 5. CUDA Graphsï¼ˆé«˜çº§ï¼‰

```python
# é¢„ç¼–è¯‘æ¨ç†å›¾ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph):
    output = model(input_tensor)
```

---

## å®æˆ˜ç¤ºä¾‹

### åŸºç¡€æ¨ç†

```python
from nanoinfer import load_checkpoint, Tokenizer, generate

# åŠ è½½æ¨¡å‹
model, config, tokenizer_path = load_checkpoint("model.pt")
tokenizer = Tokenizer(tokenizer_path)

# å•è½®æ¨ç†
prompt = "The future of AI is"
input_ids = tokenizer.encode(prompt, return_tensors="pt")
output_ids = generate(
    model=model,
    input_ids=input_ids,
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.9
)
response = tokenizer.decode(output_ids[0])
print(response)
```

### æµå¼ç”Ÿæˆ

```python
# æµå¼è¾“å‡º
for token in generate(model, input_ids, stream=True):
    token_text = tokenizer.decode(token.cpu())
    print(token_text, end="", flush=True)
```

### æ‰¹é‡æ¨ç†

```python
from nanoinfer.engine.generator import generate_batch

prompts = [
    "Explain quantum computing",
    "Write a haiku about AI",
    "What is machine learning?"
]

responses = generate_batch(
    model=model,
    prompts=prompts,
    tokenizer=tokenizer,
    max_new_tokens=100,
    batch_size=4
)
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
from nanoinfer.plugins.optimizer import benchmark_model

# æµ‹è¯•ä¸åŒé…ç½®
results = benchmark_model(
    model=model,
    input_shape=(4, 512),  # batch_size=4, seq_len=512
    num_runs=10
)

print(f"Average latency: {results['avg_latency_ms']:.2f}ms")
print(f"Throughput: {results['tokens_per_second']:.1f} tokens/sec")
```

### è¯„ä¼°æŒ‡æ ‡

```python
from nanoinfer.evaluation.metrics import compute_metrics

# åŠ è½½è¯„ä¼°æ•°æ®
eval_data = [
    {"prompt": "What is AI?", "reference": "AI is artificial intelligence..."},
    # ... æ›´å¤šæ ·æœ¬
]

# è®¡ç®—æŒ‡æ ‡
results = compute_metrics(model, tokenizer, eval_data)
print(f"Perplexity: {results['perplexity']:.2f}")
print(f"BLEU-4: {results['bleu_4']:.3f}")
print(f"ROUGE-L: {results['rougeL']:.3f}")
```

---

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆç”Ÿæˆé€Ÿåº¦æ…¢ï¼Ÿ
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. æ˜¯å¦å¯ç”¨äº† KV Cache
2. æ˜¯å¦ä½¿ç”¨äº† FP16 ç²¾åº¦
3. æ˜¯å¦åº”ç”¨äº† torch.compile
4. æ‰¹å¤„ç†å¤§å°æ˜¯å¦åˆé€‚

### Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. ä½¿ç”¨ FP16 æˆ– BF16
2. å‡å°æ‰¹å¤„ç†å¤§å°
3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
4. è€ƒè™‘æ¨¡å‹é‡åŒ–

### Q: CPU æ¨ç†å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä¼˜åŒ–ç­–ç•¥ï¼š
1. ä½¿ç”¨ FP32 è€Œä¸æ˜¯ FP16ï¼ˆCPU ä¸Š FP32 é€šå¸¸æ›´å¿«ï¼‰
2. è°ƒæ•´ CPU çº¿ç¨‹æ•°ï¼š`torch.set_num_threads(4)`
3. å¯ç”¨ torch.compile ä¼˜åŒ–
4. ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°
5. è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹

### Q: ç”Ÿæˆè´¨é‡ä¸å¥½ï¼Ÿ
A: è°ƒæ•´é‡‡æ ·å‚æ•°ï¼š
1. é™ä½ temperatureï¼ˆæ›´ç¡®å®šæ€§ï¼‰
2. è°ƒæ•´ top_p å€¼ï¼ˆ0.9-0.95ï¼‰
3. ä½¿ç”¨ top_k é™åˆ¶å€™é€‰
4. æ£€æŸ¥ prompt è´¨é‡

### Q: å¦‚ä½•æé«˜ååé‡ï¼Ÿ
A: ä¼˜åŒ–ç­–ç•¥ï¼š
1. å¢åŠ æ‰¹å¤„ç†å¤§å°
2. ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†
3. å¯ç”¨ CUDA Graphs
4. è€ƒè™‘æ¨¡å‹å¹¶è¡Œ

---

## è¿›é˜¶æŠ€å·§

### 1. åŠ¨æ€åœæ­¢æ¡ä»¶

```python
def generate_with_stop(model, input_ids, stop_tokens=[], max_tokens=200):
    for token in generate(model, input_ids, stream=True):
        if token.item() in stop_tokens:
            break
        yield token
```

### 2. æ¡ä»¶ç”Ÿæˆ

```python
def conditional_generate(model, input_ids, condition_fn):
    for token in generate(model, input_ids, stream=True):
        if condition_fn(token):
            break
        yield token
```

### 3. å¤šè½®å¯¹è¯

```python
def multi_turn_chat(model, tokenizer, conversation_history):
    # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
    context = tokenizer.encode(conversation_history)
    
    # ç”Ÿæˆå›å¤
    response = generate(model, context)
    
    return tokenizer.decode(response)
```

---

## æ€»ç»“

NanoInfer çš„è®¾è®¡å“²å­¦æ˜¯**ä»é›¶ç†è§£æ¨ç†**ï¼š

1. **é€æ˜æ€§**: æ¯ä¸ªæ­¥éª¤éƒ½å¯ä»¥è¿½è¸ªå’Œè°ƒè¯•
2. **æ•™è‚²æ€§**: ä»£ç æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†
3. **å®ç”¨æ€§**: æ”¯æŒç”Ÿäº§çº§ä¼˜åŒ–å’Œéƒ¨ç½²
4. **æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºå®šåˆ¶

é€šè¿‡è¿™ä¸ªæ•™ç¨‹ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- ç†è§£ LLM æ¨ç†çš„æ ¸å¿ƒæœºåˆ¶
- æŒæ¡ KV Cache çš„å·¥ä½œåŸç†
- é€‰æ‹©åˆé€‚çš„é‡‡æ ·ç­–ç•¥
- åº”ç”¨æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- æ„å»ºå®Œæ•´çš„æ¨ç†ç³»ç»Ÿ

è®°ä½ï¼š**ç†è§£åŸç†æ¯”è¿½æ±‚æ€§èƒ½æ›´é‡è¦**ã€‚å…ˆè®©ä»£ç å·¥ä½œï¼Œå†è®©å®ƒè·‘å¾—æ›´å¿«ã€‚
