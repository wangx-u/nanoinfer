# ğŸ§  NanoInfer  
> The smallest, clearest, and most educational inference pipeline for LLMs.  
> **From checkpoint to conversation â€” in 100 lines of code.**

---

## âœ¨ What is NanoInfer?

**NanoInfer** æ˜¯ä¸€ä¸ªæç®€æ¨ç†æ¡†æ¶ / æ•™ç¨‹ï¼Œæ—¨åœ¨ä»åº•å±‚å‰–æ **LLM æ¨ç†é˜¶æ®µ** çš„å…¨è¿‡ç¨‹ã€‚  
å®ƒç»§æ‰¿äº† [NanoChat](https://github.com/karpathy/nanochat) çš„ç†å¿µï¼š  
> â€œTrain small, understand deeply.â€

NanoInfer è®©ä½ ä»…ä½¿ç”¨ PyTorch ç†è§£ä¸€ä¸ªå®Œæ•´çš„è‡ªå›å½’æ¨ç†å¾ªç¯ï¼š  
- å¦‚ä½•ä» HFæ¨¡å‹ æˆ– `.safetensors` æƒé‡æ–‡ä»¶åŠ è½½æ¨¡å‹  
- å¦‚ä½•æ‰§è¡Œ `generate()` æ¨ç†å¾ªç¯ï¼ˆå« KV cacheï¼‰  
- å¦‚ä½•æ„å»º tokenizer + sampling  
- å¦‚ä½•åšæ¨ç†è¯„ä¼°ä¸éƒ¨ç½²  

ç›®æ ‡ï¼š**ç†è§£æ¨ç† â†’ ä¼˜åŒ–æ¨ç† â†’ æœåŠ¡æ¨ç†**

---

## ğŸ§© Features

| æ¨¡å— | åŠŸèƒ½ | æŠ€æœ¯å…³é”®è¯ |
|------|------|-------------|
| ğŸ§  Model Loader | æ”¯æŒ HuggingFace å’Œ .pt æ ¼å¼ | HuggingFace / PyTorch |
| ğŸ”¡ Tokenizer | å…¼å®¹ HuggingFace tokenizer | AutoTokenizer / BPE |
| ğŸ” Generator | è‡ªå›å½’ç”Ÿæˆ | KV Cache / Sampling |
| ğŸ“Š Evaluator | è®¡ç®— PPL / BLEU / CORE | è½»é‡æŒ‡æ ‡è®¡ç®— |
| ğŸŒ Chat Server | å¿«é€Ÿéƒ¨ç½² Web èŠå¤© | FastAPI / HuggingFace |
| âš™ï¸ Accelerator | æ€§èƒ½ä¼˜åŒ– | FP16 / Torch Compile / CUDA Graphs |

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourname/nanoinfer.git
cd nanoinfer
pip install -r requirements.txt
````

Requirements:

* Python â‰¥ 3.10
* PyTorch â‰¥ 2.1
* transformers, numpy, tqdm, fastapi, uvicorn

**æ–°å¢**: ç°åœ¨é»˜è®¤æ”¯æŒ HuggingFace æ¨¡å‹ï¼Œæ— éœ€è½¬æ¢ï¼

---

## ğŸš€ Quick Start

è¿è¡Œå•è½®æ¨ç†ï¼ˆæœ€å°ç¤ºä¾‹ï¼‰ï¼š

```bash
# ä½¿ç”¨ HuggingFace æ¨¡å‹ï¼ˆæ¨èï¼‰
python -m scripts.chat_infer --model gpt2 --prompt "Explain transformers in simple terms."

# ä½¿ç”¨ .pt æ£€æŸ¥ç‚¹æ–‡ä»¶
python -m scripts.chat_infer --model model.pt --prompt "Explain transformers in simple terms."

# CPU æ¨ç†
python -m scripts.chat_infer --model gpt2 --device cpu --dtype fp32 --prompt "Explain transformers in simple terms."
```

è¾“å‡ºï¼š

```
ğŸ§  Loading tokenizer and model...
ğŸ—£ï¸ Model output:
Transformers are neural networks that learn context by attending to all words at once.
â±ï¸ Inference time: 0.83s for 200 tokens
```

---


## ğŸ¤— HuggingFace æ¨¡å‹æ”¯æŒ

NanoInfer ç°åœ¨é»˜è®¤æ”¯æŒ HuggingFace æ¨¡å‹ï¼Œæ— éœ€ä»»ä½•è½¬æ¢ï¼

### æ”¯æŒçš„æ¨¡å‹

- **GPT-2 ç³»åˆ—**: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **DialoGPT ç³»åˆ—**: `microsoft/DialoGPT-small`, `microsoft/DialoGPT-medium`
- **å…¶ä»– GPT æ¶æ„æ¨¡å‹**

### å¿«é€Ÿä½¿ç”¨

```bash
# ç›´æ¥ä½¿ç”¨ HuggingFace æ¨¡å‹
python -m scripts.chat_infer --model gpt2 --prompt "Hello world"

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python -m scripts.chat_infer --model gpt2-medium --prompt "The future of AI"

# å¯åŠ¨ API æœåŠ¡å™¨
python -m scripts.chat_server --model gpt2 --host 0.0.0.0 --port 8000
```

### ç¼–ç¨‹æ¥å£

```python
from nanoinfer.model.loader import load_model
from nanoinfer.tokenizer.tokenizer import Tokenizer

# åŠ è½½ HuggingFace æ¨¡å‹
model, config, tokenizer_path = load_model("gpt2", device="cuda")
tokenizer = Tokenizer(tokenizer_path)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
input_ids = tokenizer.encode("Hello world", return_tensors="pt")
output = model.generate(input_ids, max_new_tokens=50)
response = tokenizer.decode(output[0])
```

---

## ğŸ§  Inference Pipeline

```python
# pseudo-code overview

from nanoinfer.model.loader import load_model
from nanoinfer.engine.generator import generate
from nanoinfer.tokenizer.tokenizer import Tokenizer

# åŠ è½½ HuggingFace æ¨¡å‹ï¼ˆæ¨èï¼‰
model, config, tokenizer_path = load_model("gpt2", device="cuda")
tokenizer = Tokenizer(tokenizer_path)

# æˆ–è€…åŠ è½½ .pt æ£€æŸ¥ç‚¹
# model, config, tokenizer_path = load_model("model.pt", device="cuda")

prompt = "The theory of attention is"
ids = tokenizer.encode(prompt, return_tensors="pt")

out = generate(model, ids, max_new_tokens=128, temperature=0.8)
print(tokenizer.decode(out[0]))
```

æ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š

* KV Cacheï¼ˆO(N) æ¨ç†ï¼‰
* Temperature / Top-p / Top-k é‡‡æ ·
* FP16 + CUDA Graphs åŠ é€Ÿ
* æ‰¹é‡ç”Ÿæˆï¼ˆBatch Decodeï¼‰

---

## ğŸ§ª Evaluation

```bash
# ä½¿ç”¨ HuggingFace æ¨¡å‹è¿›è¡Œè¯„ä¼°
torchrun --standalone --nproc_per_node=1 -m scripts.chat_eval_infer \
  --model gpt2 \
  --eval_set ~/nanobase/eval_bundle/core_eval.json

# æˆ–ä½¿ç”¨ .pt æ£€æŸ¥ç‚¹
torchrun --standalone --nproc_per_node=1 -m scripts.chat_eval_infer \
  --model ~/nanobase/models/sft_model.pt \
  --eval_set ~/nanobase/eval_bundle/core_eval.json
```

Metrics:

| æŒ‡æ ‡                   | å«ä¹‰       |
| -------------------- | -------- |
| **Perplexity (PPL)** | è¯­è¨€å»ºæ¨¡å›°æƒ‘åº¦  |
| **CORE metric**      | å¤æ‚æ¨ç†ä»»åŠ¡è¯„ä¼° |
| **BLEU / ROUGE**     | è‡ªç„¶è¯­è¨€ç”Ÿæˆè´¨é‡ |

---

## âš¡ Performance Tips

| ä¼˜åŒ–é¡¹                 | æè¿°                | GPU æ•ˆæœ      | CPU æ•ˆæœ      |
| ------------------- | ----------------- | --------- | --------- |
| **KV cache**        | ç¼“å­˜æ³¨æ„åŠ›å±‚å†å²çŠ¶æ€        | â±ï¸ 2â€“4Ã—   | â±ï¸ 2â€“3Ã—   |
| **FP16**            | åŠç²¾åº¦æ¨ç†             | ğŸ’¾ æ˜¾å­˜å‡åŠ   | âš ï¸ å¯èƒ½æ›´æ…¢   |
| **FP32**            | å•ç²¾åº¦æ¨ç†ï¼ˆCPU æ¨èï¼‰     | ğŸ’¾ æ˜¾å­˜æ›´å¤š   | ğŸš€ é€šå¸¸æ›´å¿«   |
| **torch.compile()** | PyTorch 2.x è‡ªåŠ¨å›¾ä¼˜åŒ– | ğŸš€ 20â€“40% | ğŸš€ 30â€“50% |
| **Batch Decode**    | å¤š prompt å¹¶è¡Œç”Ÿæˆ     | ğŸ” ååæå‡   | ğŸ” ååæå‡   |
| **CUDA Graphs**     | é¢„ç¼–è¯‘æ¨ç†å›¾ï¼ˆä»… GPUï¼‰    | âš™ï¸ ç¨³å®šå»¶è¿Ÿ   | âŒ ä¸æ”¯æŒ     |

---

## ğŸŒ Serve It

å¯åŠ¨äº¤äº’ç•Œé¢ï¼š

```bash
python -m scripts.chat_server
```

è®¿é—® ğŸ‘‰ [http://localhost:8000](http://localhost:8000)

ç¤ºä¾‹ï¼š

```python
POST /api/chat
{
  "prompt": "Write a haiku about GPU inference."
}
```

---

## ğŸ§­ Roadmap

* [x] åŸºç¡€æ¨ç†å¾ªç¯ (CPU/GPU)
* [ ] CORE ä»»åŠ¡è¯„ä¼°
* [ ] FastAPI æœåŠ¡æ¥å£
* [ ] CPU ä¼˜åŒ–æ”¯æŒ
* [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
* [x] Streaming ç”Ÿæˆæ¥å£
* [ ] Tensor Parallel Serving
* [ ] åŠ¨æ€ KV cache ç®¡ç†

---

## ğŸ§© Relationship to NanoChat

| é¡¹ç›®                                               | åŠŸèƒ½é˜¶æ®µ | æ ¸å¿ƒç›®æ ‡       |
| ------------------------------------------------ | ---- | ---------- |
| [NanoChat](https://github.com/karpathy/nanochat) | è®­ç»ƒ   | ä»é›¶ç†è§£è¯­è¨€æ¨¡å‹è®­ç»ƒ |
| **NanoInfer**                                    | æ¨ç†   | ä»é›¶ç†è§£è¯­è¨€æ¨¡å‹æ¨ç† |

> â€œNanoChat teaches your model to think.
> NanoInfer teaches it to speak.â€
