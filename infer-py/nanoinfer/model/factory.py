import torch
from typing import Tuple, Optional
from .base import BaseModel, HuggingFaceModel, NanoInferModel
from .loader import load_model as load_nanoinfer_model
from transformers import AutoModelForCausalLM, AutoTokenizer


def create_model(
    model_name: str,
    model_type: str = "auto",
    device: str = "cpu",
    dtype: torch.dtype = torch.float32
) -> Tuple[BaseModel, object]:
    """åˆ›å»ºæ¨¡å‹å’Œtokenizer
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ ("hf", "nanoinfer", "auto")
        device: è®¾å¤‡
        dtype: æ•°æ®ç±»å‹
        
    Returns:
        (model, tokenizer)
    """
    if model_type == "auto":
        model_type = _detect_model_type(model_name)
    
    if model_type == "hf":
        return _create_huggingface_model(model_name, device, dtype)
    elif model_type == "nanoinfer":
        return _create_nanoinfer_model(model_name, device, dtype)
    else:
        raise ValueError(f"Unknown model type: {model_type}")


def _detect_model_type(model_name: str) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹"""
    import os
    if os.path.exists(model_name) and model_name.endswith('.pt'):
        return "nanoinfer"
    else:
        return "hf"


def _create_huggingface_model(
    model_name: str, 
    device: str, 
    dtype: torch.dtype
) -> Tuple[BaseModel, object]:
    """åˆ›å»ºHuggingFaceæ¨¡å‹"""
    print(f"ğŸ“¥ Loading HuggingFace model: {model_name}")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_name, 
        dtype=dtype,
        device_map="auto" if device != "cpu" else None
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    if device != "auto":
        model = model.to(device)
    
    # åˆ›å»ºé€‚é…å™¨
    hf_model = HuggingFaceModel(model, tokenizer)
    
    print(f"âœ… HuggingFace model loaded successfully!")
    print(f"   - Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - Device: {device}")
    print(f"   - Dtype: {dtype}")
    
    return hf_model, tokenizer


def _create_nanoinfer_model(
    model_path: str, 
    device: str, 
    dtype: torch.dtype
) -> Tuple[BaseModel, object]:
    """åˆ›å»ºNanoInferæ¨¡å‹"""
    print(f"ğŸ“¥ Loading NanoInfer model: {model_path}")
    
    # ä½¿ç”¨ç°æœ‰çš„loader
    model, config, tokenizer_path = load_nanoinfer_model(
        model_path, device=device, dtype=dtype
    )
    
    # åŠ è½½tokenizer
    from ..tokenizer.tokenizer import Tokenizer
    tokenizer = Tokenizer(tokenizer_path)
    
    # åˆ›å»ºé€‚é…å™¨
    nanoinfer_model = NanoInferModel(model, config)
    
    print(f"âœ… NanoInfer model loaded successfully!")
    print(f"   - Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - Device: {device}")
    print(f"   - Dtype: {dtype}")
    
    return nanoinfer_model, tokenizer
