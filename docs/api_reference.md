# ğŸ“š NanoInfer API å‚è€ƒ

> å®Œæ•´çš„ API æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

## ç›®å½•

1. [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
2. [æ¨ç†å¼•æ“](#æ¨ç†å¼•æ“)
3. [ä¼˜åŒ–å·¥å…·](#ä¼˜åŒ–å·¥å…·)
4. [è¯„ä¼°æŒ‡æ ‡](#è¯„ä¼°æŒ‡æ ‡)
5. [è„šæœ¬å·¥å…·](#è„šæœ¬å·¥å…·)
6. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¨¡å—

### Model Loader

#### `load_checkpoint(path, device='cuda', dtype=torch.float16)`

åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚

**å‚æ•°**:
- `path` (str): æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
- `device` (str): ç›®æ ‡è®¾å¤‡ ('cuda', 'cpu', 'mps')
- `dtype` (torch.dtype): æ•°æ®ç±»å‹

**è¿”å›**:
- `model`: GPT æ¨¡å‹å®ä¾‹
- `config`: GPTConfig é…ç½®å¯¹è±¡
- `tokenizer_path`: Tokenizer è·¯å¾„

**ç¤ºä¾‹**:
```python
from nanoinfer import load_checkpoint

model, config, tokenizer_path = load_checkpoint(
    "model.pt", 
    device="cuda", 
    dtype=torch.float16
)
```

#### `GPTConfig`

æ¨¡å‹é…ç½®ç±»ã€‚

**å±æ€§**:
- `vocab_size` (int): è¯æ±‡è¡¨å¤§å°
- `n_layer` (int): Transformer å±‚æ•°
- `n_head` (int): æ³¨æ„åŠ›å¤´æ•°
- `n_embd` (int): åµŒå…¥ç»´åº¦
- `block_size` (int): ä¸Šä¸‹æ–‡é•¿åº¦

**æ–¹æ³•**:
- `from_dict(config_dict)`: ä»å­—å…¸åˆ›å»ºé…ç½®
- `to_dict()`: è½¬æ¢ä¸ºå­—å…¸

### Tokenizer

#### `Tokenizer(tokenizer_path)`

Tokenizer åŒ…è£…å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼ã€‚

**å‚æ•°**:
- `tokenizer_path` (str): Tokenizer æ–‡ä»¶è·¯å¾„

**æ–¹æ³•**:
- `encode(text, bos=True, eos=False)`: ç¼–ç æ–‡æœ¬
- `decode(tokens)`: è§£ç  token åºåˆ—
- `__call__(text, return_tensors=None)`: ç›´æ¥è°ƒç”¨

**å±æ€§**:
- `vocab_size`: è¯æ±‡è¡¨å¤§å°
- `bos_token_id`: BOS token ID
- `eos_token_id`: EOS token ID
- `pad_token_id`: PAD token ID

**ç¤ºä¾‹**:
```python
from nanoinfer import Tokenizer

tokenizer = Tokenizer("tokenizer.model")
tokens = tokenizer.encode("Hello world")
text = tokenizer.decode(tokens)
```

---

## æ¨ç†å¼•æ“

### `generate(model, input_ids, **kwargs)`

æ ¸å¿ƒæ¨ç†å‡½æ•°ï¼Œæ”¯æŒå¤šç§ç”Ÿæˆé€‰é¡¹ã€‚

**å‚æ•°**:
- `model`: GPT æ¨¡å‹
- `input_ids` (torch.Tensor): è¾“å…¥ token IDs
- `max_new_tokens` (int): æœ€å¤§ç”Ÿæˆ token æ•°
- `temperature` (float): é‡‡æ ·æ¸©åº¦
- `top_p` (float): Top-p é‡‡æ ·é˜ˆå€¼
- `top_k` (int): Top-k é‡‡æ ·
- `use_cache` (bool): æ˜¯å¦ä½¿ç”¨ KV Cache
- `stream` (bool): æ˜¯å¦æµå¼è¾“å‡º
- `stop_tokens` (List[int]): åœæ­¢ token åˆ—è¡¨

**è¿”å›**:
- ç”Ÿæˆç»“æœæˆ–ç”Ÿæˆå™¨ï¼ˆæµå¼æ¨¡å¼ï¼‰

**ç¤ºä¾‹**:
```python
from nanoinfer.engine import generate

# éæµå¼ç”Ÿæˆ
output_ids = generate(
    model=model,
    input_ids=input_ids,
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.9
)

# æµå¼ç”Ÿæˆ
for token in generate(model, input_ids, stream=True):
    print(tokenizer.decode(token), end="")
```

### é‡‡æ ·å‡½æ•°

#### `apply_temperature(logits, temperature)`

åº”ç”¨æ¸©åº¦ç¼©æ”¾ã€‚

#### `top_k_filtering(logits, k)`

Top-k è¿‡æ»¤ã€‚

#### `top_p_filtering(logits, p)`

Top-p (nucleus) è¿‡æ»¤ã€‚

#### `sample_next_token(logits, temperature, top_k, top_p)`

é‡‡æ ·ä¸‹ä¸€ä¸ª tokenã€‚

---

## ä¼˜åŒ–å·¥å…·

### `optimize_model(model, strategy, device)`

åº”ç”¨ä¼˜åŒ–ç­–ç•¥ã€‚

**å‚æ•°**:
- `model`: æ¨¡å‹å®ä¾‹
- `strategy` (str): ä¼˜åŒ–ç­–ç•¥ ('fp16', 'bf16', 'compile', 'all', 'cpu_optimized')
- `device` (str): ç›®æ ‡è®¾å¤‡

**ç¤ºä¾‹**:
```python
from nanoinfer.plugins import optimize_model

# GPU ä¼˜åŒ–
model = optimize_model(model, strategy="fp16", device="cuda")
model = optimize_model(model, strategy="compile", device="cuda")

# CPU ä¼˜åŒ–
model = optimize_model(model, strategy="cpu_optimized", device="cpu")
model = optimize_model(model, strategy="compile", device="cpu")

# å…¨éƒ¨ä¼˜åŒ–
model = optimize_model(model, strategy="all", device="cuda")
```

### `benchmark_model(model, input_shape, num_runs)`

æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚

**å‚æ•°**:
- `model`: æ¨¡å‹å®ä¾‹
- `input_shape` (tuple): è¾“å…¥å½¢çŠ¶ (batch_size, seq_len)
- `num_runs` (int): æµ‹è¯•è½®æ•°

**è¿”å›**:
- åŸºå‡†æµ‹è¯•ç»“æœå­—å…¸

### `get_memory_usage(device)`

è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µã€‚

### `clear_memory_cache(device)`

æ¸…ç†æ˜¾å­˜ç¼“å­˜ã€‚

---

## è¯„ä¼°æŒ‡æ ‡

### `compute_perplexity(model, tokenizer, text_dataset)`

è®¡ç®—å›°æƒ‘åº¦ã€‚

**å‚æ•°**:
- `model`: GPT æ¨¡å‹
- `tokenizer`: Tokenizer å®ä¾‹
- `text_dataset` (List[str]): æ–‡æœ¬æ•°æ®é›†

**è¿”å›**:
- å›°æƒ‘åº¦åˆ†æ•°

### `compute_bleu(predictions, references)`

è®¡ç®— BLEU åˆ†æ•°ã€‚

**å‚æ•°**:
- `predictions` (List[str]): é¢„æµ‹æ–‡æœ¬
- `references` (List[str]): å‚è€ƒæ–‡æœ¬

**è¿”å›**:
- BLEU åˆ†æ•°å­—å…¸

### `compute_rouge(predictions, references)`

è®¡ç®— ROUGE åˆ†æ•°ã€‚

### `compute_metrics(model, tokenizer, eval_data)`

è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡ã€‚

**å‚æ•°**:
- `model`: GPT æ¨¡å‹
- `tokenizer`: Tokenizer å®ä¾‹
- `eval_data` (List[Dict]): è¯„ä¼°æ•°æ®

**è¿”å›**:
- è¯„ä¼°ç»“æœå­—å…¸

---

## è„šæœ¬å·¥å…·

### CLI æ¨ç†

```bash
python -m scripts.chat_infer \
  --model model.pt \
  --prompt "Hello world" \
  --max_tokens 100 \
  --temperature 0.8 \
  --stream
```

### è¯„ä¼°è„šæœ¬

```bash
python -m scripts.chat_eval_infer \
  --model model.pt \
  --eval_set eval.json \
  --output results.json
```

### æ€§èƒ½åŸºå‡†

```bash
python -m scripts.chat_bench \
  --model model.pt \
  --batch_sizes 1 4 8 \
  --seq_lengths 128 512 \
  --strategies none fp16 compile
```

### FastAPI æœåŠ¡

```bash
python -m scripts.chat_server \
  --model model.pt \
  --host 0.0.0.0 \
  --port 8000
```

**API ç«¯ç‚¹**:
- `POST /api/chat`: èŠå¤©æ¥å£
- `GET /api/health`: å¥åº·æ£€æŸ¥
- `GET /api/model_info`: æ¨¡å‹ä¿¡æ¯

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€æ¨ç†æµç¨‹

```python
from nanoinfer import load_checkpoint, Tokenizer, generate

# 1. åŠ è½½æ¨¡å‹
model, config, tokenizer_path = load_checkpoint("model.pt")
tokenizer = Tokenizer(tokenizer_path)

# 2. å‡†å¤‡è¾“å…¥
prompt = "The future of AI is"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

# 3. ç”Ÿæˆ
output_ids = generate(
    model=model,
    input_ids=input_ids,
    max_new_tokens=100,
    temperature=0.8
)

# 4. è§£ç è¾“å‡º
response = tokenizer.decode(output_ids[0])
print(response)
```

### æ‰¹é‡æ¨ç†

```python
from nanoinfer.engine.generator import generate_batch

prompts = [
    "Explain quantum computing",
    "Write a haiku about AI",
    "What is machine learning?"
]

responses = generate_batch(
    model=model,
    prompts=prompts,
    tokenizer=tokenizer,
    max_new_tokens=100,
    batch_size=4
)
```

### æµå¼ç”Ÿæˆ

```python
# æµå¼è¾“å‡º
for token in generate(model, input_ids, stream=True):
    token_text = tokenizer.decode(token.cpu())
    print(token_text, end="", flush=True)
```

### æ€§èƒ½ä¼˜åŒ–

```python
from nanoinfer.plugins import optimize_for_inference

# åº”ç”¨æ‰€æœ‰ä¼˜åŒ–
model = optimize_for_inference(model, device="cuda")

# åŸºå‡†æµ‹è¯•
from nanoinfer.plugins import benchmark_model
results = benchmark_model(model, (4, 512), num_runs=10)
print(f"Throughput: {results['tokens_per_second']:.1f} tokens/sec")
```

### è¯„ä¼°æŒ‡æ ‡

```python
from nanoinfer.evaluation import compute_metrics

eval_data = [
    {"prompt": "What is AI?", "reference": "AI is artificial intelligence..."},
    # ... æ›´å¤šæ ·æœ¬
]

results = compute_metrics(model, tokenizer, eval_data)
print(f"Perplexity: {results['perplexity']:.2f}")
print(f"BLEU-4: {results['bleu_4']:.3f}")
```

### FastAPI é›†æˆ

```python
from fastapi import FastAPI
from nanoinfer import load_checkpoint, Tokenizer, generate

app = FastAPI()
model, config, tokenizer_path = load_checkpoint("model.pt")
tokenizer = Tokenizer(tokenizer_path)

@app.post("/chat")
async def chat(request: dict):
    prompt = request["prompt"]
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output_ids = generate(model, input_ids, max_new_tokens=100)
    response = tokenizer.decode(output_ids[0])
    return {"response": response}
```

---

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯

1. **CUDA å†…å­˜ä¸è¶³**
   ```python
   # è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ FP16 æˆ–å‡å°æ‰¹å¤„ç†å¤§å°
   model = model.half()
   ```

2. **Tokenizer åŠ è½½å¤±è´¥**
   ```python
   # æ£€æŸ¥è·¯å¾„å’Œæ ¼å¼
   tokenizer = Tokenizer("path/to/tokenizer.model")
   ```

3. **æ¨¡å‹é…ç½®ä¸åŒ¹é…**
   ```python
   # æ£€æŸ¥é…ç½®å‚æ•°
   print(config)
   ```

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
print(f"Model device: {next(model.parameters()).device}")
print(f"Model dtype: {next(model.parameters()).dtype}")

# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
print(f"GPU memory: {torch.cuda.memory_allocated() / 1024**2:.1f}MB")
```

---

## æœ€ä½³å®è·µ

### 1. æ¨¡å‹åŠ è½½

```python
# æ¨èï¼šä½¿ç”¨ FP16 å’Œä¼˜åŒ–
model, config, tokenizer_path = load_checkpoint(
    "model.pt", 
    device="cuda", 
    dtype=torch.float16
)
model = optimize_for_inference(model)
```

### 2. æ¨ç†é…ç½®

```python
# å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
output_ids = generate(
    model=model,
    input_ids=input_ids,
    max_new_tokens=200,
    temperature=0.8,      # é€‚ä¸­çš„éšæœºæ€§
    top_p=0.9,            # ä¿æŒå¤šæ ·æ€§
    use_cache=True        # å¯ç”¨ KV Cache
)
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†æé«˜ååé‡
responses = generate_batch(
    model=model,
    prompts=prompts,
    tokenizer=tokenizer,
    batch_size=8,         # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    max_new_tokens=100
)
```

### 4. æ€§èƒ½ç›‘æ§

```python
# å®šæœŸæ£€æŸ¥æ€§èƒ½
results = benchmark_model(model, (4, 512))
if results['tokens_per_second'] < 100:
    print("âš ï¸  Performance degradation detected")
```

---

## æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰é‡‡æ ·ç­–ç•¥

```python
def custom_sampling(logits, **kwargs):
    # å®ç°è‡ªå®šä¹‰é‡‡æ ·é€»è¾‘
    pass

# åœ¨ç”Ÿæˆä¸­ä½¿ç”¨
output_ids = generate(model, input_ids, custom_sampling=custom_sampling)
```

### è‡ªå®šä¹‰ä¼˜åŒ–å™¨

```python
def custom_optimizer(model, **kwargs):
    # å®ç°è‡ªå®šä¹‰ä¼˜åŒ–é€»è¾‘
    return model
```

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

```python
def custom_metric(predictions, references):
    # å®ç°è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
    return score
```

---

## æ€»ç»“

NanoInfer æä¾›äº†å®Œæ•´çš„ LLM æ¨ç†è§£å†³æ–¹æ¡ˆï¼š

- **ç®€å•æ˜“ç”¨**: å‡ è¡Œä»£ç å³å¯å¼€å§‹æ¨ç†
- **åŠŸèƒ½å®Œæ•´**: æ”¯æŒå„ç§é‡‡æ ·ç­–ç•¥å’Œä¼˜åŒ–
- **æ€§èƒ½ä¼˜ç§€**: å†…ç½®å¤šç§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯
- **æ‰©å±•æ€§å¼º**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºå®šåˆ¶

é€šè¿‡è¿™ä¸ª API å‚è€ƒï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š
- å¿«é€Ÿä¸Šæ‰‹ NanoInfer
- ç†è§£å„ä¸ªæ¨¡å—çš„åŠŸèƒ½
- åº”ç”¨æœ€ä½³å®è·µ
- è¿›è¡Œæ‰©å±•å¼€å‘

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ [GitHub Issues](https://github.com/wangx-u/nanoinfer/issues) æˆ–æäº¤æ–°çš„ issueã€‚
